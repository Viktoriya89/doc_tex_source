{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Caltech101_module4_VSergeyeva.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN9dBRnU2+37hjgAo2JbZXU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Viktoriya89/doc_tex_source/blob/main/Caltech101_module4_VSergeyeva.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXS-nsgoxiYn"
      },
      "source": [
        "Projet Deep Image du module 4, en ré-utilisant le TP vu en cours."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTuzeknLxchi",
        "outputId": "32ffa82d-90d7-4f3d-fc99-787dc6c4bd17"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.utils as vutils\n",
        "import torch.backends.cudnn as cudnn"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8zELI49zB3w"
      },
      "source": [
        "# Progrès\n",
        "from tqdm import tqdm\n",
        "from tqdm.autonotebook import tqdm\n",
        "#from tqdm import tqdm_notebook as tqdm\n",
        "\n",
        "#matpotlib \n",
        "#import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Modules (torch, nn, F et optim)\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_9_gBBlzh2O"
      },
      "source": [
        "Chargement des images  Caltech101, environ 10000 images 300X200 pixels. Les objets sont rangés selon 101 catégories. \n",
        "http://www.vision.caltech.edu/Image_Datasets/Caltech101/101_ObjectCategories.tar.gz\n",
        "\n",
        "Documentation pour ImageFolder: https://pytorch.org/docs/stable/torchvision/datasets.html#imagefolder\n",
        "\n",
        "Train dataset = 90% du total. Test dataset = 10%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnM0w39czYkI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}